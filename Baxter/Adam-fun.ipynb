{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GRID K520 (CNMeM is disabled, cuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "from collections import Counter\n",
    "from lasagne.utils import floatX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!wget https://s3.amazonaws.com/lasagne/recipes/datasets/coco_with_cnn_features.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "dataset = pickle.load(open('coco_with_cnn_features.pkl')) #This is a list of dictionaries\n",
    "#Split into training (60%), validation (20%) and test (20%)\n",
    "#dataset, dataset_test = train_test_split(dataset,test_size=0.4)\n",
    "#dataset_test, dataset_valid = train_test_split(dataset_test,test_size=0.5)\n",
    "#print len(dataset), len(dataset_test), len(dataset_valid)\n",
    "#X = get_my_X()\n",
    "#y = get_my_y()\n",
    "#x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "#x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test 5000\n",
      "Number of val 5000\n",
      "Number of restval 30504\n",
      "Number of train 82783\n"
     ]
    }
   ],
   "source": [
    "yo = [x['split'] for x in dataset]\n",
    "print 'Number of test', sum(x=='test' for x in yo)\n",
    "print 'Number of val', sum(x=='val' for x in yo)\n",
    "print 'Number of restval', sum(x=='restval' for x in yo)\n",
    "print 'Number of train', sum(x=='train' for x in yo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allwords = Counter()\n",
    "for item in dataset:\n",
    "    for sentence in item['sentences']:\n",
    "        allwords.update(sentence['tokens'])\n",
    "        \n",
    "vocab = [k for k, v in allwords.items() if v >= 5] #Finding words that occure more than 5 times\n",
    "vocab.insert(0, '#START#')\n",
    "vocab.append('#END#')\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)} #Dic with word as key and index as value\n",
    "index_to_word = {i: w for i, w in enumerate(vocab)} #Dic with index as key and word as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 32\n",
    "MAX_SENTENCE_LENGTH = SEQUENCE_LENGTH - 3 # 1 for image, 1 for start token, 1 for end token (This means that we can at most have a caption of length 29 + start and end token)\n",
    "BATCH_SIZE = 100\n",
    "CNN_FEATURE_SIZE = 1000\n",
    "EMBEDDING_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns a list of tuples (cnn features, list of words, image ID)\n",
    "def get_data_batch(dataset, size, split='train'): #dataset, size=batch_size(100)\n",
    "    items = []\n",
    "    \n",
    "    while len(items) < size:\n",
    "        item = random.choice(dataset) #While data less than batch_size, choose a random datapoint\n",
    "        if item['split'] != split: \n",
    "            continue\n",
    "        sentence = random.choice(item['sentences'])['tokens'] #Choose a random caption from the data point\n",
    "        if len(sentence) > MAX_SENTENCE_LENGTH: #Check that that caption is not longer than max sentence length\n",
    "            continue\n",
    "        items.append((item['cnn features'], sentence, item['cocoid'])) #Add the cnn features, sentence and id to the batch\n",
    "    \n",
    "    return items #Returns the batch which is a list consisting of tuples (it should be the same length as size?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert a list of tuples into arrays that can be fed into the network\n",
    "def prep_batch_for_network(batch):\n",
    "    x_cnn = floatX(np.zeros((len(batch), 1000))) #batch_size X 1000 (1000 is the number of cnn features)\n",
    "    x_sentence = np.zeros((len(batch), SEQUENCE_LENGTH - 1), dtype='int32') #batch_size x 31 (which is  the max length of a sentence if we include start and stop characters)\n",
    "    y_sentence = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='int32') #Why is this one longer?\n",
    "    mask = np.zeros((len(batch), SEQUENCE_LENGTH), dtype='bool') #What is mask?\n",
    "\n",
    "    for j, (cnn_features, sentence, _) in enumerate(batch):\n",
    "        x_cnn[j] = cnn_features\n",
    "        i = 0\n",
    "        for word in ['#START#'] + sentence + ['#END#']:\n",
    "            if word in word_to_index:\n",
    "                mask[j, i] = True\n",
    "                y_sentence[j, i] = word_to_index[word]\n",
    "                x_sentence[j, i] = word_to_index[word]\n",
    "                i += 1\n",
    "        mask[j, 0] = False\n",
    "                \n",
    "    return x_cnn, x_sentence, y_sentence, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentence embedding maps integer sequence with dim (BATCH_SIZE, SEQUENCE_LENGTH - 1) to \n",
    "# (BATCH_SIZE, SEQUENCE_LENGTH-1, EMBEDDING_SIZE)\n",
    "\n",
    "### ENCODER STARTS ###\n",
    "## Enocding the captions\n",
    "l_input_sentence = lasagne.layers.InputLayer((BATCH_SIZE, SEQUENCE_LENGTH - 1)) #Batch size time the length of the caption\n",
    "l_sentence_embedding = lasagne.layers.EmbeddingLayer(l_input_sentence,\n",
    "                                                     input_size=len(vocab),\n",
    "                                                     output_size=EMBEDDING_SIZE,\n",
    "                                                    )\n",
    "\n",
    "# cnn embedding changes the dimensionality of the representation from 1000 to EMBEDDING_SIZE, \n",
    "# and reshapes to add the time dimension - final dim (BATCH_SIZE, 1, EMBEDDING_SIZE)\n",
    "l_input_cnn = lasagne.layers.InputLayer((BATCH_SIZE, CNN_FEATURE_SIZE)) #Batch size times 100\n",
    "l_cnn_embedding = lasagne.layers.DenseLayer(l_input_cnn, num_units=EMBEDDING_SIZE,\n",
    "                                            nonlinearity=lasagne.nonlinearities.identity) #Resize the cnn to embdding_size (512 i belive)\n",
    "\n",
    "l_cnn_embedding = lasagne.layers.ReshapeLayer(l_cnn_embedding, ([0], 1, [1])) #Reshape the embedding to ...\n",
    "\n",
    "# the two are concatenated to form the RNN input with dim (BATCH_SIZE, SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "### DECODER STARTS##\n",
    "\n",
    "l_rnn_input = lasagne.layers.ConcatLayer([l_cnn_embedding, l_sentence_embedding]) \n",
    "\n",
    "l_dropout_input = lasagne.layers.DropoutLayer(l_rnn_input, p=0.5)\n",
    "l_lstm = lasagne.layers.LSTMLayer(l_dropout_input,\n",
    "                                  num_units=EMBEDDING_SIZE,\n",
    "                                  unroll_scan=True,\n",
    "                                  grad_clipping=5.)\n",
    "l_dropout_output = lasagne.layers.DropoutLayer(l_lstm, p=0.5)\n",
    "\n",
    "# the RNN output is reshaped to combine the batch and time dimensions\n",
    "# dim (BATCH_SIZE * SEQUENCE_LENGTH, EMBEDDING_SIZE)\n",
    "l_shp = lasagne.layers.ReshapeLayer(l_dropout_output, (-1, EMBEDDING_SIZE))\n",
    "\n",
    "# decoder is a fully connected layer with one output unit for each word in the vocabulary\n",
    "l_decoder = lasagne.layers.DenseLayer(l_shp, num_units=len(vocab), nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "# finally, the separation between batch and time dimension is restored\n",
    "l_out = lasagne.layers.ReshapeLayer(l_decoder, (BATCH_SIZE, SEQUENCE_LENGTH, len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cnn feature vector\n",
    "x_cnn_sym = T.matrix()\n",
    "\n",
    "# sentence encoded as sequence of integer word tokens\n",
    "x_sentence_sym = T.imatrix()\n",
    "\n",
    "# mask defines which elements of the sequence should be predicted\n",
    "mask_sym = T.imatrix()\n",
    "\n",
    "# ground truth for the RNN output\n",
    "y_sentence_sym = T.imatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/lasagne/layers/recurrent.py:1089: DeprecationWarning: stack(*tensors) interface is deprecated, use stack(tensors, axis=0) instead.\n",
      "  n_steps=input_shape[1])\n",
      "/usr/local/lib/python2.7/dist-packages/lasagne/layers/recurrent.py:1089: DeprecationWarning: stack(*tensors) interface is deprecated, use stack(tensors, axis=0) instead.\n",
      "  n_steps=input_shape[1])\n"
     ]
    }
   ],
   "source": [
    "output = lasagne.layers.get_output(l_out, {\n",
    "                l_input_sentence: x_sentence_sym,\n",
    "                l_input_cnn: x_cnn_sym\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cross_ent(net_output, mask, targets):\n",
    "    # Helper function to calculate the cross entropy error\n",
    "    preds = T.reshape(net_output, (-1, len(vocab)))\n",
    "    targets = T.flatten(targets)\n",
    "    cost = T.nnet.categorical_crossentropy(preds, targets)[T.flatten(mask).nonzero()]\n",
    "    return cost\n",
    "\n",
    "loss = T.mean(calc_cross_ent(output, mask_sym, y_sentence_sym))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_GRAD_NORM = 15\n",
    "\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "all_grads = T.grad(loss, all_params)\n",
    "all_grads = [T.clip(g, -5, 5) for g in all_grads]\n",
    "all_grads, norm = lasagne.updates.total_norm_constraint(\n",
    "    all_grads, MAX_GRAD_NORM, return_norm=True)\n",
    "\n",
    "updates = lasagne.updates.adam(all_grads, all_params, learning_rate=0.001)\n",
    "\n",
    "f_train = theano.function([x_cnn_sym, x_sentence_sym, mask_sym, y_sentence_sym],\n",
    "                          [loss, norm, output],\n",
    "                          updates=updates\n",
    "                         )\n",
    "\n",
    "f_val = theano.function([x_cnn_sym, x_sentence_sym, mask_sym, y_sentence_sym], [loss,output])\n",
    "\n",
    "f_eval = theano.function([x_sentence_sym,x_cnn_sym], output, on_unused_input='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#myLayers = lasagne.layers.get_all_param_values(l_out)\n",
    "#with open('30000Layers','w') as f:\n",
    "#    pickle.dump(myLayers,f)\n",
    "layers30000 = pickle.load(open('30000Layers', 'rb'))\n",
    "lasagne.layers.set_all_param_values(l_out, layers30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss_train: 2.53035902977, norm: 0.83237349987\n",
      "Val loss: 2.71643924713\n",
      "['#START#', u'an', u'empty', u'kitchen', u'with', u'a', u'window', u'and', u'a', u'refrigerators', '#END#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#', '#START#']\n",
      "[u'nook', u'a', u'empty', u'kitchen', u'with', u'a', u'sink', u'and', u'a', u'sink', '#END#', '#END#', '#END#', '#END#', '#END#', '#END#', '#END#', '#END#', '#END#', '#END#', '#END#', '#END#', '#END#', '#END#', '#END#', '#END#', u'and', u'and', '#END#', '#END#', '#END#', '#END#']\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(250):\n",
    "    x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(get_data_batch(dataset, BATCH_SIZE))\n",
    "    loss_train, norm,output = f_train(x_cnn, x_sentence, mask, y_sentence)\n",
    "    if not iteration % 250:\n",
    "        print('Iteration {}, loss_train: {}, norm: {}'.format(iteration, loss_train, norm))\n",
    "        try:\n",
    "            batch = get_data_batch(dataset, BATCH_SIZE, split='val')\n",
    "            x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(batch)\n",
    "            loss_val,output_val = f_val(x_cnn, x_sentence, mask, y_sentence)\n",
    "            print('Val loss: {}'.format(loss_val))\n",
    "            image_nr = 1\n",
    "            print [index_to_word[x] for x in x_sentence[image_nr]]\n",
    "            print [index_to_word[np.argmax(x)] for x in output_val[image_nr]]\n",
    "        except IndexError:\n",
    "            continue    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "def imgPlotter(fileName):\n",
    "    if fileName[0:10]=='COCO_train':\n",
    "        fileDir = \"../coco/train2014\"\n",
    "    if fileName[0:8]=='COCO_val' or fileName[0:9]=='COCO_test':\n",
    "        fileDir = \"../coco/val2014\"\n",
    "    img = io.imread('%s/%s'%(fileDir,fileName))\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#myInputBatch = get_data_batch(dataset, BATCH_SIZE, split='test')\n",
    "#print myInputBatch[1][1]\n",
    "#print myInputBatch[1][2]\n",
    "#theOne = [x for x in dataset if x['cocoid']==myInputBatch[99][2]]\n",
    "#x_cnn, x_sentence, y_sentence, mask = prep_batch_for_network(myInputBatch)\n",
    "#x_cnn.shape\n",
    "#f_eval(np.zeros((BATCH_SIZE, SEQUENCE_LENGTH - 1), dtype='int32'), x_cnn).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x_cnn):\n",
    "    wordsList = ['']*BATCH_SIZE\n",
    "    x_sentence = np.zeros((BATCH_SIZE, SEQUENCE_LENGTH - 1), dtype='int32')\n",
    "    continueBool = [True]*BATCH_SIZE\n",
    "    i = 0\n",
    "    words = [[]]*BATCH_SIZE\n",
    "    while any(continueBool):\n",
    "        i += 1\n",
    "        p0 = f_eval(x_sentence, x_cnn)\n",
    "        pa = p0.argmax(-1)\n",
    "        for j in range(BATCH_SIZE):\n",
    "            if continueBool[j]:\n",
    "                tok = pa[j,:][i]\n",
    "                word = index_to_word[tok]\n",
    "                if word == '#END#' or i >= SEQUENCE_LENGTH - 1:\n",
    "                    wordsList[j] = (' '.join(words[j]))\n",
    "                    continueBool[j]=False\n",
    "                else:\n",
    "                    x_sentence[j][i] = tok\n",
    "                    if word != '#START#':\n",
    "                        words[j] = words[j]+[word]\n",
    "    return wordsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30385\n"
     ]
    }
   ],
   "source": [
    "dataBatchTest = []\n",
    "continueBool = True\n",
    "for i in range(len(dataset)):\n",
    "    x = dataset[i]\n",
    "    if continueBool and x['split']=='restval' and max([len(y['tokens']) for y in x['sentences']])<=MAX_SENTENCE_LENGTH:\n",
    "        for j in range(len(x['sentences'])):\n",
    "            if len(x['sentences'][j]['tokens'])<=MAX_SENTENCE_LENGTH:\n",
    "                sentence = x['sentences'][j]['tokens']\n",
    "        dataBatchTest.append((x['cnn features'], sentence, x['cocoid']))\n",
    "#dataBatchTest = dataBatchTest[0:(len(dataBatchTest)-(len(dataBatchTest)%100))]\n",
    "print(len(dataBatchTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.3300330033\n",
      "0.660066006601\n",
      "0.990099009901\n",
      "1.3201320132\n",
      "1.6501650165\n",
      "1.9801980198\n",
      "2.3102310231\n",
      "2.6402640264\n",
      "2.9702970297\n",
      "3.300330033\n",
      "3.6303630363\n",
      "3.9603960396\n",
      "4.2904290429\n",
      "4.6204620462\n",
      "4.9504950495\n",
      "5.28052805281\n",
      "5.61056105611\n",
      "5.94059405941\n",
      "6.27062706271\n",
      "6.60066006601\n",
      "6.93069306931\n",
      "7.26072607261\n",
      "7.59075907591\n",
      "7.92079207921\n",
      "8.25082508251\n",
      "8.58085808581\n",
      "8.91089108911\n",
      "9.24092409241\n",
      "9.57095709571\n",
      "9.90099009901\n",
      "10.2310231023\n",
      "10.5610561056\n",
      "10.8910891089\n",
      "11.2211221122\n",
      "11.5511551155\n",
      "11.8811881188\n",
      "12.2112211221\n",
      "12.5412541254\n",
      "12.8712871287\n",
      "13.201320132\n",
      "13.5313531353\n",
      "13.8613861386\n",
      "14.1914191419\n",
      "14.5214521452\n",
      "14.8514851485\n",
      "15.1815181518\n",
      "15.5115511551\n",
      "15.8415841584\n",
      "16.1716171617\n",
      "16.501650165\n",
      "16.8316831683\n",
      "17.1617161716\n",
      "17.4917491749\n",
      "17.8217821782\n",
      "18.1518151815\n",
      "18.4818481848\n",
      "18.8118811881\n",
      "19.1419141914\n",
      "19.4719471947\n",
      "19.801980198\n",
      "20.1320132013\n",
      "20.4620462046\n",
      "20.7920792079\n",
      "21.1221122112\n",
      "21.4521452145\n",
      "21.7821782178\n",
      "22.1122112211\n",
      "22.4422442244\n",
      "22.7722772277\n",
      "23.102310231\n",
      "23.4323432343\n",
      "23.7623762376\n",
      "24.0924092409\n",
      "24.4224422442\n",
      "24.7524752475\n",
      "25.0825082508\n",
      "25.4125412541\n",
      "25.7425742574\n",
      "26.0726072607\n",
      "26.402640264\n",
      "26.7326732673\n",
      "27.0627062706\n",
      "27.3927392739\n",
      "27.7227722772\n",
      "28.0528052805\n",
      "28.3828382838\n",
      "28.7128712871\n",
      "29.0429042904\n",
      "29.3729372937\n",
      "29.702970297\n",
      "30.0330033003\n",
      "30.3630363036\n",
      "30.6930693069\n",
      "31.0231023102\n",
      "31.3531353135\n",
      "31.6831683168\n",
      "32.0132013201\n",
      "32.3432343234\n",
      "32.6732673267\n",
      "33.00330033\n",
      "33.3333333333\n",
      "33.6633663366\n",
      "33.9933993399\n",
      "34.3234323432\n",
      "34.6534653465\n",
      "34.9834983498\n",
      "35.3135313531\n",
      "35.6435643564\n",
      "35.9735973597\n",
      "36.303630363\n",
      "36.6336633663\n",
      "36.9636963696\n",
      "37.2937293729\n",
      "37.6237623762\n",
      "37.9537953795\n",
      "38.2838283828\n",
      "38.6138613861\n",
      "38.9438943894\n",
      "39.2739273927\n",
      "39.603960396\n",
      "39.9339933993\n",
      "40.2640264026\n",
      "40.5940594059\n",
      "40.9240924092\n",
      "41.2541254125\n",
      "41.5841584158\n",
      "41.9141914191\n",
      "42.2442244224\n",
      "42.5742574257\n",
      "42.904290429\n",
      "43.2343234323\n",
      "43.5643564356\n",
      "43.8943894389\n",
      "44.2244224422\n",
      "44.5544554455\n",
      "44.8844884488\n",
      "45.2145214521\n",
      "45.5445544554\n",
      "45.8745874587\n",
      "46.204620462\n",
      "46.5346534653\n",
      "46.8646864686\n",
      "47.1947194719\n",
      "47.5247524752\n",
      "47.8547854785\n",
      "48.1848184818\n",
      "48.5148514851\n",
      "48.8448844884\n",
      "49.1749174917\n",
      "49.504950495\n",
      "49.8349834983\n",
      "50.1650165017\n",
      "50.495049505\n",
      "50.8250825083\n",
      "51.1551155116\n",
      "51.4851485149\n",
      "51.8151815182\n",
      "52.1452145215\n",
      "52.4752475248\n",
      "52.8052805281\n",
      "53.1353135314\n",
      "53.4653465347\n",
      "53.795379538\n",
      "54.1254125413\n",
      "54.4554455446\n",
      "54.7854785479\n",
      "55.1155115512\n",
      "55.4455445545\n",
      "55.7755775578\n",
      "56.1056105611\n",
      "56.4356435644\n",
      "56.7656765677\n",
      "57.095709571\n",
      "57.4257425743\n",
      "57.7557755776\n",
      "58.0858085809\n",
      "58.4158415842\n",
      "58.7458745875\n",
      "59.0759075908\n",
      "59.4059405941\n",
      "59.7359735974\n",
      "60.0660066007\n",
      "60.396039604\n",
      "60.7260726073\n",
      "61.0561056106\n",
      "61.3861386139\n",
      "61.7161716172\n",
      "62.0462046205\n",
      "62.3762376238\n",
      "62.7062706271\n",
      "63.0363036304\n",
      "63.3663366337\n",
      "63.696369637\n",
      "64.0264026403\n",
      "64.3564356436\n",
      "64.6864686469\n",
      "65.0165016502\n",
      "65.3465346535\n",
      "65.6765676568\n",
      "66.0066006601\n",
      "66.3366336634\n",
      "66.6666666667\n",
      "66.99669967\n",
      "67.3267326733\n",
      "67.6567656766\n",
      "67.9867986799\n",
      "68.3168316832\n",
      "68.6468646865\n",
      "68.9768976898\n",
      "69.3069306931\n",
      "69.6369636964\n",
      "69.9669966997\n",
      "70.297029703\n",
      "70.6270627063\n",
      "70.9570957096\n",
      "71.2871287129\n",
      "71.6171617162\n",
      "71.9471947195\n",
      "72.2772277228\n",
      "72.6072607261\n",
      "72.9372937294\n",
      "73.2673267327\n",
      "73.597359736\n",
      "73.9273927393\n",
      "74.2574257426\n",
      "74.5874587459\n",
      "74.9174917492\n",
      "75.2475247525\n",
      "75.5775577558\n",
      "75.9075907591\n",
      "76.2376237624\n",
      "76.5676567657\n",
      "76.897689769\n",
      "77.2277227723\n",
      "77.5577557756\n",
      "77.8877887789\n",
      "78.2178217822\n",
      "78.5478547855\n",
      "78.8778877888\n",
      "79.2079207921\n",
      "79.5379537954\n",
      "79.8679867987\n",
      "80.198019802\n",
      "80.5280528053\n",
      "80.8580858086\n",
      "81.1881188119\n",
      "81.5181518152\n",
      "81.8481848185\n",
      "82.1782178218\n",
      "82.5082508251\n",
      "82.8382838284\n",
      "83.1683168317\n",
      "83.498349835\n",
      "83.8283828383\n",
      "84.1584158416\n",
      "84.4884488449\n",
      "84.8184818482\n",
      "85.1485148515\n",
      "85.4785478548\n",
      "85.8085808581\n",
      "86.1386138614\n",
      "86.4686468647\n",
      "86.798679868\n",
      "87.1287128713\n",
      "87.4587458746\n",
      "87.7887788779\n",
      "88.1188118812\n",
      "88.4488448845\n",
      "88.7788778878\n",
      "89.1089108911\n",
      "89.4389438944\n",
      "89.7689768977\n",
      "90.099009901\n",
      "90.4290429043\n",
      "90.7590759076\n",
      "91.0891089109\n",
      "91.4191419142\n",
      "91.7491749175\n",
      "92.0792079208\n",
      "92.4092409241\n",
      "92.7392739274\n",
      "93.0693069307\n",
      "93.399339934\n",
      "93.7293729373\n",
      "94.0594059406\n",
      "94.3894389439\n",
      "94.7194719472\n",
      "95.0495049505\n",
      "95.3795379538\n",
      "95.7095709571\n",
      "96.0396039604\n",
      "96.3696369637\n",
      "96.699669967\n",
      "97.0297029703\n",
      "97.3597359736\n",
      "97.6897689769\n",
      "98.0198019802\n",
      "98.3498349835\n",
      "98.6798679868\n",
      "99.0099009901\n",
      "99.3399339934\n",
      "99.6699669967\n"
     ]
    }
   ],
   "source": [
    "allTestPreds = []\n",
    "for i in range(len(dataBatchTest)/BATCH_SIZE):#(len(dataBatchTest)/BATCH_SIZE):\n",
    "    print (i/float(len(dataBatchTest)/BATCH_SIZE)*100)\n",
    "    littleDataBatchTest = dataBatchTest[(i*BATCH_SIZE):((i+1)*BATCH_SIZE)]\n",
    "    imgIds = [x[2] for x in littleDataBatchTest]\n",
    "    x_cnn, null1, null2, null3 = prep_batch_for_network(littleDataBatchTest)#np.zeros((BATCH_SIZE,1000))\n",
    "    for j in range(BATCH_SIZE):\n",
    "        x_cnn[j,] = littleDataBatchTest[j][0]\n",
    "    sentences = predict(x_cnn)\n",
    "    for j in range(BATCH_SIZE):\n",
    "        allTestPreds.append({'image_id':imgIds[j] , 'caption':sentences[j]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for j in range(80,100):\n",
    "#    print(allTestPreds[j]['caption'])\n",
    "#    print allTestPreds[j]\n",
    "#    try:\n",
    "#        imgPlotter([x['filename'] for x in dataset if x['cocoid']==allTestPreds[j]['image_id']][0])\n",
    "#    except:\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30300\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('coco-caption/results/captions_val2014_trainedMoreRestval_results.json', 'w') as outfile:\n",
    "    json.dump(allTestPreds, outfile)\n",
    "with open('coco-caption/results/captions_val2014_trainedMoreRestval_results.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "    print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
